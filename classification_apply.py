import argparse
import csv
import json
import pickle
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

csv.field_size_limit(1000000000)

parser = argparse.ArgumentParser()
parser.add_argument(
    "--model",
    dest="model",
    required=True,
    help="Model file generated by training script."
)
parser.add_argument(
    "--data",
    dest="data",
    required=True,
    help="Data file model will be applied to."
)
parser.add_argument(
    "--counts",
    dest="counts",
    required=True,
    help="Counts output file."
)
parser.add_argument(
    "--target_field",
    dest="target_field",
    required=True,
    help="The CSV field that the classifier will learn to recognize."
)
parser.add_argument(
    "--content_field",
    dest="content_field",
    default="full_content",
    help="The CSV field containing the primary text content"
)
parser.add_argument(
    "--subdocument_length",
    dest="subdocument_length",
    default=200,
    type=int,
    help="The number of tokens to have in each subdocument."
)
args = parser.parse_args()


with open(args.model, "rb") as ifd:
    label_encoder, count_vectorizer, model, _ = pickle.loads(ifd.read())

with open(args.data, "rt") as ifd:
    cifd = csv.DictReader(ifd,delimiter="\t")
    docs = []
    items = []
    targets = []
    for row in cifd:
        tokens = row[args.content_field].split()
        target = row[args.target_field]
        if target == "":
            continue
        num_subdocs = int(len(tokens)/args.subdocument_length)
        for subnum in range(num_subdocs):
            start = subnum * args.subdocument_length
            end = (subnum+1) * args.subdocument_length
            sub_tokens = tokens[start:end]
            sub_document = " ".join(sub_tokens)
            docs.append(sub_document)
            items.append({k : v for k, v in row.items() if k != args.content_field})
            targets.append(target)

X = count_vectorizer.transform(docs)
predictions = label_encoder.inverse_transform(model.predict(X))

with open(args.counts, "wt") as ofd:
    ofd.write(
        json.dumps(
            [
                {
                    "item" : t,
                    "target" : a,
                    "prediction" : p,
                    "content" : d} for t, a, p, d in zip(
                        items,
                        targets,
                        predictions,
                        docs
                    )
            ],
            indent=4
        )
    ) 
